# Import necessary libraries (Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktar)
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer

# NLTK araÃ§larÄ±nÄ± indirme (Bu adÄ±m bir defalÄ±k gereklidir)
nltk.download('punkt')
nltk.download('punkt_tab')


# Example 1: Word Tokenization (Kelime Tokenizasyonu)
# Tokenizing a sentence into individual words.
# Bir cÃ¼mleyi kelimelere ayÄ±rma.
text = "Natural Language Processing (NLP) is fascinating!"

# Basic word tokenization using NLTK
# NLTK kullanarak temel kelime tokenizasyonu
words = word_tokenize(text)
print(f"Word Tokenization: {words}")  # ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fascinating', '!']

# Example 2: Sentence Tokenization (CÃ¼mle Tokenizasyonu)
# Tokenizing text into individual sentences.
# Bir metni cÃ¼mlelere ayÄ±rma.
text = "Hello world. Welcome to the field of NLP. Let's explore tokenization."

# Sentence tokenization using NLTK
# NLTK kullanarak cÃ¼mle tokenizasyonu
sentences = sent_tokenize(text)
print(f"Sentence Tokenization: {sentences}")
# ['Hello world.', 'Welcome to the field of NLP.', "Let's explore tokenization."]

# Example 3: Tokenization using Regular Expressions (DÃ¼zenli Ä°fadelerle Tokenizasyon)
# Tokenization based on custom patterns.
# Ã–zel kalÄ±plara gÃ¶re tokenizasyon.

text = "The price of the book is $19.99. Contact us at support@example.com."

# Regex-based tokenization: extract words, prices, and email addresses
# DÃ¼zenli ifadeye dayalÄ± tokenizasyon: kelimeleri, fiyatlarÄ± ve e-posta adreslerini ayÄ±rma
pattern = r'\w+|\$[\d\.]+|\S+@\S+'
tokens = regexp_tokenize(text, pattern)
print(f"Regex Tokenization: {tokens}")
# ['The', 'price', 'of', 'the', 'book', 'is', '$19.99', '.', 'Contact', 'us', 'at', 'support@example.com', '.']

# Example 4: Tokenizing Tweets and Informal Text (Tweet ve Resmi Olmayan Metinlerin Tokenizasyonu)
# Tokenizing tweets or short messages where informal language is common.
# Gayri resmi dilin yaygÄ±n olduÄŸu tweetlerin veya kÄ±sa mesajlarÄ±n tokenizasyonu.

tweet = "I love #NLP! ğŸ˜ Let's tokenize this tweet @example #AI"

# Using TweetTokenizer from NLTK
# NLTK'deki TweetTokenizer'Ä± kullanma
tweet_tokenizer = TweetTokenizer()
tweet_tokens = tweet_tokenizer.tokenize(tweet)
print(f"Tweet Tokenization: {tweet_tokens}")
# ['I', 'love', '#NLP', 'ğŸ˜', "Let's", 'tokenize', 'this', 'tweet', '@example', '#AI']

# Example 5: Custom Tokenization Rules (Ã–zelleÅŸtirilmiÅŸ Tokenizasyon KurallarÄ±)
# You can create custom tokenizers for specific use cases.
# Ã–zel kullanÄ±m durumlarÄ± iÃ§in Ã¶zelleÅŸtirilmiÅŸ tokenizasyon kurallarÄ± oluÅŸturabilirsiniz.

# A custom tokenizer for splitting on spaces, keeping punctuation as separate tokens.
# BoÅŸluklara gÃ¶re ayÄ±ran, noktalama iÅŸaretlerini ayrÄ± tokenlar olarak tutan bir Ã¶zelleÅŸtirilmiÅŸ tokenizer.
text = "Welcome to the world of NLP, where things move fast!"

custom_tokens = text.split()
print(f"Custom Tokenization: {custom_tokens}")
# ['Welcome', 'to', 'the', 'world', 'of', 'NLP,', 'where', 'things', 'move', 'fast!']

# Example 6: Handling Multi-Word Expressions (Ã‡ok Kelimeli Ä°fadelerin Ä°ÅŸlenmesi)
# Sometimes you may want to treat multi-word expressions as a single token.
# Bazen Ã§ok kelimeli ifadeleri tek bir token olarak ele almak isteyebilirsiniz.

# Example of multi-word expressions like "New York" or "San Francisco".
# "New York" veya "San Francisco" gibi Ã§ok kelimeli ifadelerle Ã¶rnek.
multiword_text = "I live in New York and work in San Francisco."

# Defining a pattern to capture multi-word expressions
# Ã‡ok kelimeli ifadeleri yakalamak iÃ§in bir kalÄ±p tanÄ±mlama
multiword_pattern = r'\bNew York\b|\bSan Francisco\b|\w+'
multiword_tokens = regexp_tokenize(multiword_text, multiword_pattern)
print(f"Multi-Word Tokenization: {multiword_tokens}")
# ['I', 'live', 'in', 'New York', 'and', 'work', 'in', 'San Francisco', '.']

# Example 7: Subword Tokenization (Alt Kelime Tokenizasyonu)
# Tokenizing into parts of words or subwords, useful in some machine learning models.
# Kelimeleri alt parÃ§alara veya alt kelimelere ayÄ±rma, bazÄ± makine Ã¶ÄŸrenmesi modellerinde kullanÄ±ÅŸlÄ±dÄ±r.

from nltk.tokenize import SyllableTokenizer
text = "naturalization"

# Using NLTK's SyllableTokenizer
# NLTK'nin SyllableTokenizer'Ä±nÄ± kullanma
syllable_tokenizer = SyllableTokenizer()
syllables = syllable_tokenizer.tokenize(text)
print(f"Syllable Tokenization: {syllables}")
# ['na', 'tu', 'ra', 'li', 'za', 'tion']

# Example 8: Byte-Pair Encoding (BPE) - Alt Kelime Tokenizasyonu
# BPE is a subword tokenization algorithm often used in large language models.
# BPE, bÃ¼yÃ¼k dil modellerinde sÄ±klÄ±kla kullanÄ±lan bir alt kelime tokenizasyon algoritmasÄ±dÄ±r.

# This would typically be done using libraries such as Hugging Face's `transformers`.
# Genellikle Hugging Face'in `transformers` gibi kÃ¼tÃ¼phanelerle yapÄ±lÄ±r.
# Here's a pseudo-code example to illustrate the idea (Bu, fikri gÃ¶stermek iÃ§in bir psÃ¶do-koddur):


from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokens = tokenizer.tokenize("natural language processing")
print(tokens)


# Output: ['natural', 'language', 'processing'] -> ['nat', '##ural', 'lang', '##uage', 'process', '##ing']

# This example shows how BPE splits words into smaller subwords or character sequences.
# Bu Ã¶rnek, BPE'nin kelimeleri daha kÃ¼Ã§Ã¼k alt kelimelere veya karakter dizilerine nasÄ±l bÃ¶ldÃ¼ÄŸÃ¼nÃ¼ gÃ¶sterir.
